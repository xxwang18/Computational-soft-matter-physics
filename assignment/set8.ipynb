{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 10: Machine learning the properties of a particle in a harmonic potential\n",
    "\n",
    "Use deep learning (DL) to extract the spring constant and the friction coefficient of a noisy os\u0002cillator from particle-displacement trajectories x(t), which you can generate using the code from\n",
    "the previous assignment. The idea is similar to well-established image processing algorithms\n",
    "based on convolutional neural networks.\n",
    "First, generate a number of independent particle-displacement trajectories x(t) for different\n",
    "values of ks and γ. You can simply choose them randomly from the intervals ks ∈ [10, 100] and\n",
    "γ ∈ [10, 100]. It is advisable to save these trajectories into a file, because you can use them\n",
    "later as training and test sets (e.g., as 70% and 30% fractions of the data) for your DL model.\n",
    "No need to save every time step of x(t), for example, every 10 time steps would be sufficient.\n",
    "Second, build a DL model by using ”keras” module from tensorflow with python implementa\u0002tion. Below is an example of such a model with two convolutional, two max-pooling, and one\n",
    "dense (fully connected) layers.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(layers.Conv1D(filters=*, kernel size=*, strides = *, activation=’relu’, input shape=(*,*)))\n",
    "\n",
    "model.add(layers.MaxPooling1D(pool size=*))\n",
    "\n",
    "model.add(layers.Conv1D(filters=*, kernel size=*, strides = *, activation=’relu’))\n",
    "\n",
    "model.add(layers.MaxPooling1D(pool size=*))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(*, activation=’relu’))\n",
    "\n",
    "model.add(layers.Dense(2, activation=’relu’))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from scipy.optimize import curve_fit\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def create_particle(x0,particle_list):\n",
    "    particle_list.append(x0)\n",
    "\n",
    "def force(gamma,ks,x,x0,v,dt):\n",
    "    xi = np.random.normal()\n",
    "    return -gamma*v-ks*(x-x0)+2*np.sqrt(gamma)*xi/np.sqrt(dt)\n",
    "\n",
    "def velocity_verlet(x,v,dt,m,gamma,ks,x0):\n",
    "    f_t = force(gamma,ks,x,x0,v,dt)\n",
    "    v_half = v+dt/2*f_t/m\n",
    "    x_next = x+dt*v_half\n",
    "    f_next = force(gamma,ks,x_next,x0,v_half,dt)\n",
    "    v_next = v_half+dt/2*f_next/m\n",
    "    return x_next,v_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x, v = 0, 0, 0\n",
    "dt, m = 0.01, 1\n",
    "sim_time = 100\n",
    "data_num = 70\n",
    "\n",
    "for num in range(data_num):\n",
    "    gamma,ks = np.random.uniform(10,100),np.random.uniform(10,100)\n",
    "    data = []\n",
    "    traj_list = []\n",
    "    for i in range(int(sim_time / dt)):\n",
    "        if i % 10 == 0:  # 每10个时间步长\n",
    "            traj_list.append(x)\n",
    "        x, v = velocity_verlet(x, v, dt, m, gamma, ks, x0)\n",
    "\n",
    "    time_list = np.arange(0, 100, 0.1)\n",
    "\n",
    "    # 添加 gamma 和 ks 到 data 的第一行\n",
    "    data.append([gamma, ks])\n",
    "    #plt.plot(time_list, traj_list, '.', label=r'$k_s$=' + str(ks))\n",
    "    data=np.vstack((data,np.column_stack((time_list, traj_list))))\n",
    "    np.savetxt('./set8/trainning/'+str(num)+'.dat', data, header='gamma\\tks', delimiter='\\t', fmt='%10.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, x, v = 0, 0, 0\n",
    "dt, m = 0.01, 1\n",
    "sim_time = 100\n",
    "data_num = 30\n",
    "\n",
    "for num in range(data_num):\n",
    "    gamma,ks = np.random.uniform(10,100),np.random.uniform(10,100)\n",
    "    data = []\n",
    "    traj_list = []\n",
    "    for i in range(int(sim_time / dt)):\n",
    "        if i % 10 == 0:  # 每10个时间步长\n",
    "            traj_list.append(x)\n",
    "        x, v = velocity_verlet(x, v, dt, m, gamma, ks, x0)\n",
    "\n",
    "    time_list = np.arange(0, 100, 0.1)\n",
    "\n",
    "    # 添加 gamma 和 ks 到 data 的第一行\n",
    "    data.append([gamma, ks])\n",
    "    #plt.plot(time_list, traj_list, '.', label=r'$k_s$=' + str(ks))\n",
    "    data=np.vstack((data,np.column_stack((time_list, traj_list))))\n",
    "    np.savetxt('./set8/test/'+str(num)+'.dat', data, header='gamma\\tks', delimiter='\\t', fmt='%10.5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Rest of your code...\n",
    "\n",
    "# Split trajectories into training and test sets\n",
    "train_fraction = 0.7\n",
    "train_size = int(train_fraction * num_trajectories)\n",
    "train_set = trajectories[:train_size]\n",
    "test_set = trajectories[train_size:]\n",
    "\n",
    "# Pad the trajectories to have the same length\n",
    "train_set = pad_sequences(train_set, padding='post')\n",
    "test_set = pad_sequences(test_set, padding='post')\n",
    "\n",
    "# Build the DL model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=3, strides=1, activation='relu', input_shape=(train_set.shape[1], 1)))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='relu'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_set, train_set, epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(test_set, test_set)\n",
    "print('Test Loss:', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1470, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 2 and 10000 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential_1/dense_3/Relu, mean_squared_error/remove_squeezable_dimensions/Squeeze)' with input shapes: [?,2], [?,10000].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 84\u001b[0m\n\u001b[0;32m     81\u001b[0m model\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     83\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m model\u001b[39m.\u001b[39;49mfit(train_set, train_set, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n\u001b[0;32m     86\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[0;32m     87\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(test_set, test_set)\n",
      "File \u001b[1;32mc:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\XIONGX~1\\AppData\\Local\\Temp\\__autograph_generated_files4aw7tkm.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\Xiongxiao Wang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\losses.py\", line 1470, in mean_squared_error\n        return backend.mean(tf.math.squared_difference(y_pred, y_true), axis=-1)\n\n    ValueError: Dimensions must be equal, but are 2 and 10000 for '{{node mean_squared_error/SquaredDifference}} = SquaredDifference[T=DT_FLOAT](sequential_1/dense_3/Relu, mean_squared_error/remove_squeezable_dimensions/Squeeze)' with input shapes: [?,2], [?,10000].\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def force(gamma,ks,x,x0,v,dt):\n",
    "    xi = np.random.normal()\n",
    "    return -gamma*v-ks*(x-x0)+2*np.sqrt(gamma)*xi/np.sqrt(dt)\n",
    "\n",
    "def velocity_verlet(x,v,dt,m,gamma,ks,x0):\n",
    "    f_t = force(gamma,ks,x,x0,v,dt)\n",
    "    v_half = v+dt/2*f_t/m\n",
    "    x_next = x+dt*v_half\n",
    "    f_next = force(gamma,ks,x_next,x0,v_half,dt)\n",
    "    v_next = v_half+dt/2*f_next/m\n",
    "    return x_next,v_next\n",
    "\n",
    "x0, x, v = 0, 0, 0\n",
    "dt, m = 0.01, 1\n",
    "\n",
    "# Generate particle-displacement trajectories\n",
    "def generate_trajectories(num_trajectories, num_time_steps):\n",
    "    trajectories = []\n",
    "    ks_values = np.random.uniform(10, 100, num_trajectories)\n",
    "    gamma_values = np.random.uniform(10, 100, num_trajectories)\n",
    "    x0, x, v = 0, 0, 0\n",
    "    dt, m = 0.01, 1\n",
    "\n",
    "    for i in range(num_trajectories):\n",
    "        ks = ks_values[i]\n",
    "        gamma = gamma_values[i]\n",
    "        trajectory = []\n",
    "\n",
    "        for t in range(int(num_time_steps/dt)):\n",
    "            # Generate random particle displacement\n",
    "            if i%10 == 0:\n",
    "                trajectory.append(x)\n",
    "            x, v = velocity_verlet(x, v, dt, m, gamma, ks, x0)\n",
    "\n",
    "        trajectories.append(trajectory)\n",
    "\n",
    "    return np.array(trajectories, dtype=object)\n",
    "\n",
    "# Generate particle-displacement trajectories\n",
    "num_trajectories = 100\n",
    "sim_time = 100\n",
    "num_time_steps = int(sim_time/dt)\n",
    "trajectories = generate_trajectories(num_trajectories, sim_time)\n",
    "# Save trajectories to a file\n",
    "np.save('trajectories.npy', trajectories)\n",
    "\n",
    "# Split trajectories into training and test sets\n",
    "train_fraction = 0.7\n",
    "train_size = int(train_fraction * num_trajectories)\n",
    "train_set = trajectories[:train_size]\n",
    "test_set = trajectories[train_size:]\n",
    "print(train_set.shape[0])\n",
    "# Build the DL model\n",
    "max_trajectory_length = max(len(trajectory) for trajectory in trajectories)\n",
    "padded_trajectories = pad_sequences(trajectories, maxlen=max_trajectory_length, padding='post', dtype=np.float32)\n",
    "train_set = padded_trajectories[:train_size]\n",
    "test_set = padded_trajectories[train_size:]\n",
    "\n",
    "# Reshape the data for compatibility with Conv1D input shape\n",
    "train_set = train_set.reshape(train_set.shape[0], train_set.shape[1], 1)\n",
    "test_set = test_set.reshape(test_set.shape[0], test_set.shape[1], 1)\n",
    "\n",
    "# Build the DL model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Conv1D(filters=32, kernel_size=3, strides=1, activation='relu', input_shape=(train_set.shape[1], 1)))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, strides=1, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(2, activation='relu'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_set[1], train_set[0], epochs=10, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(test_set, test_set)\n",
    "print('Test Loss:', loss)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 79ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(test_set)\n",
    "test_set[0]\n",
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
